# Papers
1. [[PDF](https://arxiv.org/pdf/1710.10196.pdf)] [[Blog](https://towardsdatascience.com/progan-how-nvidia-generated-images-of-unprecedented-quality-51c98ec2cbd2?fbclid=IwAR3Fa-jvvre2VNxDR-uIe_tGISXh1OGQ_UIrYcrXVgi1uLjdQ-180cVLpX4)] [[Official Code](https://github.com/tkarras/progressive_growing_of_gans)] [[Talk](https://www.youtube.com/watch?v=ReZiqCybQPA)] Progressive Growing of GANs for Improved Quality, Stability, and Variation, ICLR 2018
2. [[PDF](http://science.sciencemag.org/content/sci/360/6394/1204.full.pdf)] [[Blog](https://deepmind.com/blog/neural-scene-representation-and-rendering/)] Neural scene representation and rendering
3. [[PDF](https://arxiv.org/pdf/1611.07004.pdf)] Image-to-Image Translation with Conditional Adversarial Networks
4. [[PDF](https://arxiv.org/pdf/1810.05795.pdf)] Point GAN
5. [[PDF](https://arxiv.org/pdf/1512.09300.pdf)] Autoencoding beyond pixels using a learned similarity metric
6. [[PDF](https://arxiv.org/pdf/1609.05566.pdf)] Label-Free Supervision of Neural Networks with Physics and Domain Knowledge
7. [[PDF](https://arxiv.org/pdf/1505.04597.pdf)] [[Blog 1](http://deeplearning.net/tutorial/unet.html)] [[Blog 2](https://medium.com/@keremturgutlu/semantic-segmentation-u-net-part-1-d8d6f6005066)] [[Code](https://github.com/milesial/Pytorch-UNet)] U-Net
 Skip Connections, Deconv explained
   


# Blogs
1. [Deep Learning: Perturbations and Diversity is All You Need](https://medium.com/intuitionmachine/deep-learning-perturbations-is-all-you-need-d630b6980587?fbclid=IwAR168X_cmBnZ81UOTem5ngRgirnGegKVPEt7RRN1Sdcdn8ohVr2uIIqomDg)
2. [37 Reasons why your Neural Network is not working](https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607)
3. [Understanding Categorical Cross-Entropy Loss, Binary Cross-Entropy Loss, Softmax Loss, Logistic Loss, Focal Loss and all those confusing names](https://gombru.github.io/2018/05/23/cross_entropy_loss/)
4. [From GAN to WGAN](https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html)
	All about GAN losses.